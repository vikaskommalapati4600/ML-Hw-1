{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNFSVTYc9ja1",
        "outputId": "d34d42fa-6135-4732-db7c-fc4bfefe7c9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Heuristic            Max Depth  Train Accuracy  Test Accuracy   Train Error Rate   Test Error Rate\n",
            "------------------------------------------------------------------------------------------\n",
            "entropy              1          0.880800        0.875200        0.119200           0.124800       \n",
            "entropy              2          0.894000        0.888600        0.106000           0.111400       \n",
            "entropy              3          0.899400        0.891800        0.100600           0.108200       \n",
            "entropy              4          0.920000        0.874000        0.080000           0.126000       \n",
            "entropy              5          0.937600        0.863800        0.062400           0.136200       \n",
            "entropy              6          0.952000        0.852200        0.048000           0.147800       \n",
            "entropy              7          0.962800        0.842800        0.037200           0.157200       \n",
            "entropy              8          0.970800        0.836200        0.029200           0.163800       \n",
            "entropy              9          0.977800        0.831400        0.022200           0.168600       \n",
            "entropy              10         0.981800        0.825400        0.018200           0.174600       \n",
            "entropy              11         0.985000        0.824400        0.015000           0.175600       \n",
            "entropy              12         0.986400        0.821200        0.013600           0.178800       \n",
            "entropy              13         0.986800        0.818600        0.013200           0.181400       \n",
            "entropy              14         0.986800        0.817000        0.013200           0.183000       \n",
            "entropy              15         0.986800        0.817000        0.013200           0.183000       \n",
            "entropy              16         0.986800        0.817000        0.013200           0.183000       \n",
            "majority_error       1          0.891200        0.883400        0.108800           0.116600       \n",
            "majority_error       2          0.895800        0.891200        0.104200           0.108800       \n",
            "majority_error       3          0.903400        0.886000        0.096600           0.114000       \n",
            "majority_error       4          0.914400        0.872800        0.085600           0.127200       \n",
            "majority_error       5          0.931400        0.862200        0.068600           0.137800       \n",
            "majority_error       6          0.936600        0.852200        0.063400           0.147800       \n",
            "majority_error       7          0.941200        0.843800        0.058800           0.156200       \n",
            "majority_error       8          0.946600        0.833200        0.053400           0.166800       \n",
            "majority_error       9          0.950600        0.826800        0.049400           0.173200       \n",
            "majority_error       10         0.955400        0.817200        0.044600           0.182800       \n",
            "majority_error       11         0.960400        0.803000        0.039600           0.197000       \n",
            "majority_error       12         0.967600        0.790600        0.032400           0.209400       \n",
            "majority_error       13         0.973800        0.775400        0.026200           0.224600       \n",
            "majority_error       14         0.979800        0.761800        0.020200           0.238200       \n",
            "majority_error       15         0.983400        0.758400        0.016600           0.241600       \n",
            "majority_error       16         0.986800        0.755400        0.013200           0.244600       \n",
            "gini_index           1          0.891200        0.883400        0.108800           0.116600       \n",
            "gini_index           2          0.895800        0.891200        0.104200           0.108800       \n",
            "gini_index           3          0.906400        0.884600        0.093600           0.115400       \n",
            "gini_index           4          0.924600        0.869400        0.075400           0.130600       \n",
            "gini_index           5          0.939600        0.854800        0.060400           0.145200       \n",
            "gini_index           6          0.951600        0.840200        0.048400           0.159800       \n",
            "gini_index           7          0.963600        0.828600        0.036400           0.171400       \n",
            "gini_index           8          0.973200        0.822600        0.026800           0.177400       \n",
            "gini_index           9          0.978400        0.820800        0.021600           0.179200       \n",
            "gini_index           10         0.982600        0.816600        0.017400           0.183400       \n",
            "gini_index           11         0.986000        0.812000        0.014000           0.188000       \n",
            "gini_index           12         0.986400        0.811000        0.013600           0.189000       \n",
            "gini_index           13         0.986800        0.809800        0.013200           0.190200       \n",
            "gini_index           14         0.986800        0.807600        0.013200           0.192400       \n",
            "gini_index           15         0.986800        0.807600        0.013200           0.192400       \n",
            "gini_index           16         0.986800        0.807600        0.013200           0.192400       \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def read_and_preprocess_no_replace_unknown(file_path, numerical_columns, columns):\n",
        "\n",
        "    data = pd.read_csv(file_path, header=None, names=columns)\n",
        "\n",
        "\n",
        "    for column in numerical_columns:\n",
        "        median = data[column].median()\n",
        "        data[column] = data[column].apply(lambda x: 1 if float(x) > median else 0)\n",
        "\n",
        "    return data\n",
        "\n",
        "def compute_entropy(labels):\n",
        "    values, counts = np.unique(labels, return_counts=True)\n",
        "    probabilities = counts / len(labels)\n",
        "    return -np.sum([p * np.log2(p) for p in probabilities if p > 0])\n",
        "\n",
        "def compute_majority_error(labels):\n",
        "    _, counts = np.unique(labels, return_counts=True)\n",
        "    return 1 - np.max(counts) / np.sum(counts)\n",
        "\n",
        "def compute_gini_index(labels):\n",
        "    _, counts = np.unique(labels, return_counts=True)\n",
        "    probabilities = counts / len(labels)\n",
        "    return 1 - np.sum(probabilities ** 2)\n",
        "\n",
        "def calculate_best_split(data, attribute, target_attribute, heuristic):\n",
        "    total_samples = len(data)\n",
        "    attribute_values, counts = np.unique(data[attribute], return_counts=True)\n",
        "\n",
        "    if heuristic == 'entropy':\n",
        "        total_entropy = compute_entropy(data[target_attribute])\n",
        "        weighted_entropy = np.sum([(counts[i] / total_samples) * compute_entropy(data[data[attribute] == attribute_values[i]][target_attribute]) for i in range(len(attribute_values))])\n",
        "        return total_entropy - weighted_entropy\n",
        "\n",
        "    elif heuristic == 'majority_error':\n",
        "        total_majority_error = compute_majority_error(data[target_attribute])\n",
        "        weighted_majority_error = np.sum([(counts[i] / total_samples) * compute_majority_error(data[data[attribute] == attribute_values[i]][target_attribute]) for i in range(len(attribute_values))])\n",
        "        return total_majority_error - weighted_majority_error\n",
        "\n",
        "    elif heuristic == 'gini_index':\n",
        "        total_gini = compute_gini_index(data[target_attribute])\n",
        "        weighted_gini = np.sum([(counts[i] / total_samples) * compute_gini_index(data[data[attribute] == attribute_values[i]][target_attribute]) for i in range(len(attribute_values))])\n",
        "        return total_gini - weighted_gini\n",
        "\n",
        "def build_decision_tree(data, original_data, features, target_attribute, heuristic, max_depth, current_depth=0):\n",
        "    if len(np.unique(data[target_attribute])) == 1:\n",
        "        return np.unique(data[target_attribute])[0]\n",
        "    elif len(data) == 0:\n",
        "        return np.unique(original_data[target_attribute])[np.argmax(np.unique(original_data[target_attribute], return_counts=True)[1])]\n",
        "    elif len(features) == 0 or current_depth == max_depth:\n",
        "        return np.unique(data[target_attribute])[np.argmax(np.unique(data[target_attribute], return_counts=True)[1])]\n",
        "    else:\n",
        "        heuristic_values = [calculate_best_split(data, feature, target_attribute, heuristic) for feature in features]\n",
        "        best_feature_index = np.argmax(heuristic_values)\n",
        "        best_feature = features[best_feature_index]\n",
        "\n",
        "        tree = {best_feature: {}}\n",
        "        features = [f for f in features if f != best_feature]\n",
        "\n",
        "        for value in np.unique(data[best_feature]):\n",
        "            subset_data = data.where(data[best_feature] == value).dropna()\n",
        "            subtree = build_decision_tree(subset_data, original_data, features, target_attribute, heuristic, max_depth, current_depth + 1)\n",
        "            tree[best_feature][value] = subtree\n",
        "        return tree\n",
        "\n",
        "def make_prediction(query, tree, default=None):\n",
        "    for key in list(query.keys()):\n",
        "        if key in tree.keys():\n",
        "            try:\n",
        "                result = tree[key][query[key]]\n",
        "            except:\n",
        "                return default\n",
        "            if isinstance(result, dict):\n",
        "                return make_prediction(query, result)\n",
        "            else:\n",
        "                return result\n",
        "\n",
        "def get_accuracy(data, tree):\n",
        "    queries = data.iloc[:, :-1].to_dict(orient=\"records\")\n",
        "    predictions = pd.Series([make_prediction(query, tree, 1) for query in queries])\n",
        "    return (predictions == data.iloc[:, -1]).mean()\n",
        "\n",
        "def experiment_with_trees(train_data, test_data, max_depths, heuristics):\n",
        "    results = []\n",
        "    features = ['age', 'job', 'marital', 'education', 'default', 'balance', 'housing', 'loan', 'contact', 'day', 'month', 'duration', 'campaign', 'pdays', 'previous', 'poutcome']\n",
        "\n",
        "    for heuristic in heuristics:\n",
        "        for max_depth in max_depths:\n",
        "            tree = build_decision_tree(train_data, train_data, features, 'y', heuristic, max_depth)\n",
        "            train_accuracy = get_accuracy(train_data, tree)\n",
        "            test_accuracy = get_accuracy(test_data, tree)\n",
        "            train_error = 1 - train_accuracy\n",
        "            test_error = 1 - test_accuracy\n",
        "            results.append((heuristic, max_depth, train_accuracy, test_accuracy, train_error, test_error))\n",
        "\n",
        "    return results\n",
        "\n",
        "columns = [ \"age\", \"job\", \"marital\", \"education\", \"default\", \"balance\", \"housing\", \"loan\", \"contact\", \"day\", \"month\", \"duration\", \"campaign\", \"pdays\", \"previous\", \"poutcome\", \"y\" ]\n",
        "numerical_columns = [\"age\", \"balance\", \"day\", \"duration\", \"campaign\", \"pdays\", \"previous\"]\n",
        "\n",
        "train_data = read_and_preprocess_no_replace_unknown('train.csv', numerical_columns, columns)\n",
        "test_data = read_and_preprocess_no_replace_unknown('test.csv', numerical_columns, columns)\n",
        "\n",
        "label_mapping = {'yes': 1, 'no': 0}\n",
        "train_data['y'] = train_data['y'].map(label_mapping)\n",
        "test_data['y'] = test_data['y'].map(label_mapping)\n",
        "\n",
        "max_depths = [ 1, 2, 3, 4, 5, 6, 7 , 8 ,9, 10, 11, 12, 13, 14, 15, 16]\n",
        "heuristics = ['entropy', 'majority_error', 'gini_index']\n",
        "\n",
        "experiment_results = experiment_with_trees(train_data, test_data, max_depths, heuristics)\n",
        "\n",
        "print(f\"{'Heuristic':<20} {'Max Depth':<10} {'Train Accuracy':<15} {'Test Accuracy':<15} {'Train Error Rate':<18} {'Test Error Rate':<15}\")\n",
        "print(\"-\" * 90)\n",
        "for result in experiment_results:\n",
        "    heuristic, max_depth, train_acc, test_acc, train_err, test_err = result\n",
        "    print(f\"{heuristic:<20} {max_depth:<10} {train_acc:<15.6f} {test_acc:<15.6f} {train_err:<18.6f} {test_err:<15.6f}\")"
      ]
    }
  ]
}