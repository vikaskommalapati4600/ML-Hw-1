{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiT-6SSjdWb8",
        "outputId": "cb96dd4c-9588-4641-eb6f-8b4fb7f1ada0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Heuristic            Max Depth  Train Accuracy  Test Accuracy   Train Error Rate   Test Error Rate\n",
            "------------------------------------------------------------------------------------------\n",
            "information_gain     1          0.698000        0.703297        0.302000           0.296703       \n",
            "information_gain     2          0.778000        0.777473        0.222000           0.222527       \n",
            "information_gain     3          0.819000        0.803571        0.181000           0.196429       \n",
            "information_gain     4          0.918000        0.848901        0.082000           0.151099       \n",
            "information_gain     5          0.973000        0.890110        0.027000           0.109890       \n",
            "information_gain     6          1.000000        0.875000        0.000000           0.125000       \n",
            "majority_error       1          0.698000        0.703297        0.302000           0.296703       \n",
            "majority_error       2          0.708000        0.686813        0.292000           0.313187       \n",
            "majority_error       3          0.807000        0.798077        0.193000           0.201923       \n",
            "majority_error       4          0.889000        0.822802        0.111000           0.177198       \n",
            "majority_error       5          0.964000        0.854396        0.036000           0.145604       \n",
            "majority_error       6          1.000000        0.840659        0.000000           0.159341       \n",
            "gini_index           1          0.698000        0.703297        0.302000           0.296703       \n",
            "gini_index           2          0.778000        0.777473        0.222000           0.222527       \n",
            "gini_index           3          0.824000        0.815934        0.176000           0.184066       \n",
            "gini_index           4          0.911000        0.862637        0.089000           0.137363       \n",
            "gini_index           5          0.973000        0.890110        0.027000           0.109890       \n",
            "gini_index           6          1.000000        0.875000        0.000000           0.125000       \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Function to compute entropy (used for information gain)\n",
        "def compute_entropy(labels):\n",
        "    values, counts = np.unique(labels, return_counts=True)\n",
        "    probabilities = counts / len(labels)\n",
        "    return -np.sum([p * np.log2(p) for p in probabilities if p > 0])\n",
        "\n",
        "# Function to calculate majority error (what percentage of the dataset is not the majority class)\n",
        "def compute_majority_error(labels):\n",
        "    _, counts = np.unique(labels, return_counts=True)\n",
        "    return 1 - np.max(counts) / np.sum(counts)\n",
        "\n",
        "# Function to calculate the Gini Index (measures impurity)\n",
        "def compute_gini_index(labels):\n",
        "    _, counts = np.unique(labels, return_counts=True)\n",
        "    probabilities = counts / len(labels)\n",
        "    return 1 - np.sum(probabilities ** 2)\n",
        "\n",
        "# Function to figure out the best heuristic based on user choice\n",
        "def calculate_best_split(data, attribute, target_attribute, heuristic):\n",
        "    total_samples = len(data)\n",
        "    attribute_values, counts = np.unique(data[attribute], return_counts=True)\n",
        "\n",
        "    if heuristic == 'information_gain':\n",
        "        total_entropy = compute_entropy(data[target_attribute])\n",
        "        weighted_entropy = np.sum([(counts[i] / total_samples) * compute_entropy(data[data[attribute] == attribute_values[i]][target_attribute]) for i in range(len(attribute_values))])\n",
        "        return total_entropy - weighted_entropy\n",
        "\n",
        "    elif heuristic == 'majority_error':\n",
        "        total_majority_error = compute_majority_error(data[target_attribute])\n",
        "        weighted_majority_error = np.sum([(counts[i] / total_samples) * compute_majority_error(data[data[attribute] == attribute_values[i]][target_attribute]) for i in range(len(attribute_values))])\n",
        "        return total_majority_error - weighted_majority_error\n",
        "\n",
        "    elif heuristic == 'gini_index':\n",
        "        total_gini = compute_gini_index(data[target_attribute])\n",
        "        weighted_gini = np.sum([(counts[i] / total_samples) * compute_gini_index(data[data[attribute] == attribute_values[i]][target_attribute]) for i in range(len(attribute_values))])\n",
        "        return total_gini - weighted_gini\n",
        "\n",
        "# The ID3 algorithm to create a decision tree based on the selected heuristic and max depth\n",
        "def build_decision_tree(data, original_data, features, target_attribute, heuristic, max_depth, current_depth=0):\n",
        "    # Stop when all the labels are the same\n",
        "    if len(np.unique(data[target_attribute])) == 1:\n",
        "        return np.unique(data[target_attribute])[0]\n",
        "\n",
        "    # If we have no more data, return the most common label from the original data\n",
        "    elif len(data) == 0:\n",
        "        return np.unique(original_data[target_attribute])[np.argmax(np.unique(original_data[target_attribute], return_counts=True)[1])]\n",
        "\n",
        "    # If we've hit the max depth or there are no more features to split on\n",
        "    elif len(features) == 0 or current_depth == max_depth:\n",
        "        return np.unique(data[target_attribute])[np.argmax(np.unique(data[target_attribute], return_counts=True)[1])]\n",
        "\n",
        "    # Otherwise, continue building the tree\n",
        "    else:\n",
        "        most_common_label = np.unique(data[target_attribute])[np.argmax(np.unique(data[target_attribute], return_counts=True)[1])]\n",
        "        heuristic_values = [calculate_best_split(data, feature, target_attribute, heuristic) for feature in features]\n",
        "        best_feature_index = np.argmax(heuristic_values)\n",
        "        best_feature = features[best_feature_index]\n",
        "\n",
        "        # Create a new tree node\n",
        "        tree = {best_feature: {}}\n",
        "        features = [f for f in features if f != best_feature]\n",
        "\n",
        "        # Recursively build the subtrees for each value of the best feature\n",
        "        for value in np.unique(data[best_feature]):\n",
        "            subset_data = data.where(data[best_feature] == value).dropna()\n",
        "            subtree = build_decision_tree(subset_data, original_data, features, target_attribute, heuristic, max_depth, current_depth + 1)\n",
        "            tree[best_feature][value] = subtree\n",
        "        return tree\n",
        "\n",
        "# Function to make predictions using the decision tree\n",
        "def make_prediction(query, tree, default=None):\n",
        "    for key in list(query.keys()):\n",
        "        if key in tree.keys():\n",
        "            try:\n",
        "                result = tree[key][query[key]]\n",
        "            except:\n",
        "                return default\n",
        "            # If the result is still a dictionary, we need to go deeper in the tree\n",
        "            if isinstance(result, dict):\n",
        "                return make_prediction(query, result)\n",
        "            else:\n",
        "                return result\n",
        "\n",
        "# Function to calculate the accuracy of the predictions on a dataset\n",
        "def get_accuracy(data, tree):\n",
        "    queries = data.iloc[:, :-1].to_dict(orient=\"records\")\n",
        "    predictions = pd.Series([make_prediction(query, tree, 1) for query in queries])\n",
        "    return (predictions == data.iloc[:, -1]).mean()\n",
        "\n",
        "# Train and test decision trees for different depths and heuristics\n",
        "def experiment_with_trees(train_data, test_data, max_depths, heuristics):\n",
        "    results = []\n",
        "    features = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety']\n",
        "\n",
        "    for heuristic in heuristics:\n",
        "        for max_depth in max_depths:\n",
        "            tree = build_decision_tree(train_data, train_data, features, 'label', heuristic, max_depth)\n",
        "            train_accuracy = get_accuracy(train_data, tree)\n",
        "            test_accuracy = get_accuracy(test_data, tree)\n",
        "            train_error = 1 - train_accuracy\n",
        "            test_error = 1 - test_accuracy\n",
        "            results.append((heuristic, max_depth, train_accuracy, test_accuracy, train_error, test_error))\n",
        "\n",
        "    return results\n",
        "\n",
        "# Load the training and testing datasets\n",
        "train_data = pd.read_csv('train.csv', header=None)\n",
        "test_data = pd.read_csv('test.csv', header=None)\n",
        "\n",
        "# Assign column names\n",
        "train_data.columns = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'label']\n",
        "test_data.columns = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'label']\n",
        "\n",
        "# Convert label categories to numerical values\n",
        "label_mapping = {'unacc': 0, 'acc': 1, 'good': 2, 'vgood': 3}\n",
        "train_data['label'] = train_data['label'].map(label_mapping)\n",
        "test_data['label'] = test_data['label'].map(label_mapping)\n",
        "\n",
        "# Define the depths and heuristics to test\n",
        "max_depths = [1, 2, 3, 4, 5, 6]\n",
        "heuristics = ['information_gain', 'majority_error', 'gini_index']\n",
        "\n",
        "# Run the experiments\n",
        "experiment_results = experiment_with_trees(train_data, test_data, max_depths, heuristics)\n",
        "\n",
        "# Print the results in a formatted way using basic Python print functions\n",
        "print(f\"{'Heuristic':<20} {'Max Depth':<10} {'Train Accuracy':<15} {'Test Accuracy':<15} {'Train Error Rate':<18} {'Test Error Rate':<15}\")\n",
        "print(\"-\" * 90)\n",
        "for result in experiment_results:\n",
        "    heuristic, max_depth, train_acc, test_acc, train_err, test_err = result\n",
        "    print(f\"{heuristic:<20} {max_depth:<10} {train_acc:<15.6f} {test_acc:<15.6f} {train_err:<18.6f} {test_err:<15.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KmQA3PPpeRbz"
      }
    }
  ]
}